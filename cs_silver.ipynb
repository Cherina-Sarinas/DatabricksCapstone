{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5628d9-0de2-438b-927f-edaed72ab037",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "declaration of functions used"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col,regexp_replace,when,explode_outer,from_json, lit,to_date,to_timestamp,split,size, explode, udf\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3377a3c2-5052-40fb-8e6d-5caf072f3fe6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingestion from bronze_members to silver_members table"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.expect_or_drop(\"memberid_not_null\",\"MemberID IS NOT NULL\")\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_members_view\",\n",
    "    comment=\"Cleaned members data with quality checks\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def silver_members():\n",
    "   df = spark.read.table(\"capstone.medisure.bronze_members\")\n",
    "   return df.dropDuplicates([\"MemberID\"])\n",
    "\n",
    "# Quarantine → invalid rows having memberid equal to null\n",
    "@dlt.table(\n",
    "    name=\"silver_members_quarantine\",\n",
    "    comment=\"Members rows that failed quality checks (null MemberID)\"\n",
    ")\n",
    "def silver_members_quarantine():\n",
    "    df = spark.read.table(\"capstone.medisure.bronze_members\")\n",
    "    return df.filter(col(\"MemberID\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9eeec6c-01c3-4edf-869d-c2d75b89d70a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingestion from bronze_diagnosis_ref to silver_diagnosis_ref table"
    }
   },
   "outputs": [],
   "source": [
    "#Silver Table with expectation of Code not being null\n",
    "@dlt.expect_or_drop(\"Code_not_null\", \"Code IS NOT NULL\")\n",
    "@dlt.table(\n",
    "    name=\"silver_diagnosis_ref_view\",\n",
    "    comment=\"Cleaned diagnosis_ref data with Code enforced as integer format if decimal\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def silver_diagnosis_ref():\n",
    "    df = spark.read.table(\"capstone.medisure.bronze_diagnosis_ref\")\n",
    "    return df\n",
    "\n",
    "# Quarantine → invalid rows having code equal to null\n",
    "@dlt.table(\n",
    "    name=\"silver_diagnosis_ref_quarantine\",\n",
    "    comment=\"Diagnosis reference rows that failed quality checks (null Code)\"\n",
    ")\n",
    "def silver_diagnosis_ref_quarantine():\n",
    "    df = spark.read.table(\"capstone.medisure.bronze_diagnosis_ref\")\n",
    "    return df.filter(col(\"Code\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85f155d7-712c-41a3-9818-979143e4ea21",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingestion from bronze_providers to silver_providers table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "location_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"Address\", StringType(), True),\n",
    "        StructField(\"City\", StringType(), True),\n",
    "        StructField(\"State\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "@dlt.expect_or_drop(\"providerid_not_null\", \"ProviderID IS NOT NULL\")\n",
    "@dlt.table(\n",
    "    name=\"silver_providers_view\",\n",
    "    comment=\"Normalized provider directory with flattened locations and specialties\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def silver_providers():\n",
    "    df = spark.read.table(\"capstone.medisure.bronze_providers\")\n",
    "\n",
    "    # Parse JSON string into array<struct>\n",
    "    df = df.withColumn(\"Locations\", from_json(col(\"Locations\"), location_schema)) \\\n",
    "                .withColumn(\"Specialties\", from_json(col(\"Specialties\"), ArrayType(StringType())))\n",
    "\n",
    "    # Explode parsed arrays\n",
    "    df = (\n",
    "        df.withColumn(\"location\", explode_outer(\"Locations\"))\n",
    "                .withColumn(\"Specialty\", explode_outer(\"Specialties\"))\n",
    "                .select(\n",
    "                    \"ProviderID\",\n",
    "                    \"Name\",\n",
    "                    \"TIN\",\n",
    "                    \"IsActive\",\n",
    "                    \"LastVerified\",\n",
    "                    col(\"location.Address\").alias(\"Address\"),\n",
    "                    col(\"location.City\").alias(\"City\"),\n",
    "                    col(\"location.State\").alias(\"State\"),\n",
    "                    \"Specialty\"\n",
    "                )\n",
    "    )\n",
    "\n",
    "    return df.dropDuplicates([\"ProviderID\", \"Address\", \"Specialty\"])\n",
    "\n",
    "\n",
    "# Quarantine → invalid rows having providerid equal to null\n",
    "@dlt.table(\n",
    "    name=\"silver_providers_quarantine\",\n",
    "    comment=\"Providers rows that failed quality checks (null ProviderID)\"\n",
    ")\n",
    "def silver_providers_quarantine():\n",
    "    df = spark.read.table(\"capstone.medisure.bronze_providers\")\n",
    "    return df.filter(col(\"ProviderID\").isNull())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d3d8313-c4d8-4fa9-ae79-6c2f88ee40a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ingestion from bronze_claims_batch and bronze_claims_streams tables to silver_claims_batch and silver_claims_streams table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bronze Claims Stream → View\n",
    "@dlt.view(\n",
    "    name=\"silver_claims_stream\",\n",
    "    comment=\"Intermediate cleaned claims stream data view aligned to batch schema\"\n",
    ")\n",
    "def silver_claims_view():\n",
    "    return (\n",
    "        spark.read.table(\"capstone.medisure.bronze_claims_stream\")\n",
    "        # Add missing columns that exist in batch but not in stream\n",
    "        .withColumn(\"ServiceDate\", lit(None).cast(\"date\"))\n",
    "        .withColumn(\"ClaimType\", lit(None).cast(\"string\"))\n",
    "        .withColumn(\"SubmissionChannel\", lit(None).cast(\"string\"))\n",
    "        .withColumn(\"Notes\", lit(None).cast(\"string\"))\n",
    "        # Cast ClaimDate to proper date\n",
    "        .withColumn(\"ClaimDate\", to_date(col(\"ClaimDate\"), \"yyyy-MM-dd\"))\n",
    "        #rename EventTimestamp to IngestionTimeStamp\n",
    "        .withColumn(\"IngestTimestamp\", to_timestamp(\"EventTimestamp\"))\n",
    "        .drop(\"EventTimestamp\") \n",
    "    )\n",
    "\n",
    "# Bronze Claims Batch → View\n",
    "@dlt.view(\n",
    "    name=\"silver_claims_batch\",\n",
    "    comment=\"Intermediate cleaned claims batch data view\"\n",
    ")\n",
    "def silver_claims_batch():\n",
    "    return (\n",
    "        spark.read.table(\"capstone.medisure.bronze_claims_batch\")\n",
    "        .withColumn(\"ClaimDate\", to_date(col(\"ClaimDate\"), \"yyyy-MM-dd\"))\n",
    "    )\n",
    "\n",
    "\n",
    "# Union View (Batch + Stream Claims View)\n",
    "@dlt.view(\n",
    "    name=\"silver_claims_union\",\n",
    "    comment=\"Union of batch and stream claims\"\n",
    ")\n",
    "def silver_claims_union():\n",
    "    batch_df = dlt.read(\"silver_claims_batch\")\n",
    "    stream_df = dlt.read(\"silver_claims_stream\")\n",
    "    return batch_df.unionByName(stream_df)\n",
    "\n",
    "#quarantine all invalid records of claims before the insertion in the final silver claims table\n",
    "@dlt.table(\n",
    "    name=\"silver_claims_quarantine\",\n",
    "    comment=\"Claims that failed validation or FK checks\",\n",
    "    table_properties={\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def quarantine_claims():\n",
    "    df = dlt.read(\"silver_claims_union\")\n",
    "\n",
    "    # Deduplicate\n",
    "    df = df.dropDuplicates([\"ClaimID\", \"MemberID\", \"ProviderID\"])\n",
    "\n",
    "    # Data type enforcement\n",
    "    df = df.withColumn(\"Amount\", col(\"Amount\").cast(\"decimal(12,2)\")) \\\n",
    "           .withColumn(\"ICD10Codes\", split(col(\"ICD10Codes\"), \";\")) \\\n",
    "           .withColumn(\"CPTCodes\", split(col(\"CPTCodes\"), \";\"))\n",
    "\n",
    "    # FK validation\n",
    "    members = dlt.read(\"silver_members_view\").select(\"MemberID\")\n",
    "    providers = dlt.read(\"silver_providers_view\").select(\"ProviderID\")\n",
    "    df = df.join(members, \"MemberID\", \"left\").join(providers, \"ProviderID\", \"left\")\n",
    "\n",
    "    # Filter records failing validations\n",
    "    df_quarantine = df.filter(\n",
    "        (col(\"ClaimID\").isNull()) |\n",
    "        (col(\"MemberID\").isNull()) |\n",
    "        (col(\"ProviderID\").isNull())\n",
    "    ).withColumn(\n",
    "        \"QuarantineReason\",\n",
    "        when(col(\"ClaimID\").isNull(), lit(\"Missing ClaimID\"))\n",
    "        .when(col(\"MemberID\").isNull(), lit(\"Invalid MemberID\"))\n",
    "        .when(col(\"ProviderID\").isNull(), lit(\"Invalid ProviderID\"))\n",
    "    )\n",
    "\n",
    "    return df_quarantine\n",
    "\n",
    "\n",
    "#insertion of data in silver_claims table   \n",
    "@dlt.table(\n",
    "    name=\"silver_claims_view\",\n",
    "    comment=\"Cleaned claims with FK validation, fraud scoring; invalid records sent to quarantine\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "def silver_claims():\n",
    "    df = dlt.read(\"silver_claims_union\")\n",
    "\n",
    "    # # Deduplicate\n",
    "    # df = df.dropDuplicates([\"ClaimID\", \"MemberID\", \"ProviderID\"])\n",
    "\n",
    "    # Data type enforcement\n",
    "    df = df.withColumn(\"Amount\", col(\"Amount\").cast(\"decimal(12,2)\")) \\\n",
    "           .withColumn(\"ICD10Codes\", split(col(\"ICD10Codes\"), \";\")) \\\n",
    "           .withColumn(\"CPTCodes\", split(col(\"CPTCodes\"), \";\"))\n",
    "\n",
    "    # FK validation\n",
    "    members = (\n",
    "        dlt.read(\"silver_members_view\")\n",
    "        .select(\n",
    "            \"MemberID\",\n",
    "            col(\"Name\").alias(\"MemberName\"),\n",
    "            col(\"PlanType\").alias(\"MemberPlanType\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    providers = (\n",
    "        dlt.read(\"silver_providers_view\")\n",
    "        .select(\n",
    "            \"ProviderID\",\n",
    "            col(\"Name\").alias(\"ProviderName\"),\n",
    "            col(\"Specialty\").alias(\"ProviderSpecialties\")\n",
    "        )\n",
    "    )\n",
    "    df = df.join(members, \"MemberID\", \"left\").join(providers, \"ProviderID\", \"left\")\n",
    "\n",
    "    # Filter valid id's only\n",
    "    df = df.filter(\n",
    "        (col(\"ClaimID\").isNotNull()) &\n",
    "        (col(\"MemberID\").isNotNull()) &\n",
    "        (col(\"ProviderID\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Fraud scoring UDF with incremental scoring, the more score of the record get, the higher the fraud risk\n",
    "    def base_fraud_score(amount, cpt_codes, submission_channel, service_date, claim_date):\n",
    "        score = 0\n",
    "        if amount is not None and amount > 10000:\n",
    "            score += 5\n",
    "        if cpt_codes is not None and len(cpt_codes) > 5:\n",
    "            score += 3\n",
    "        if submission_channel == \"Paper\" and submission_channel is None:\n",
    "            score += 2\n",
    "        if service_date is not None and claim_date is not None and service_date > claim_date:\n",
    "            score += 4\n",
    "        return score\n",
    "    \n",
    "    # Register the UDF\n",
    "    fraud_udf = udf(base_fraud_score, IntegerType())\n",
    "\n",
    "    # Fraud scoring: apply UDF and Mapping of numeric score to HIGH / MEDIUM / LOW using CASE/WHEN\n",
    "    df = df.withColumn(\n",
    "        \"FraudScoreNumeric\",\n",
    "        fraud_udf(\n",
    "            col(\"Amount\"),\n",
    "            col(\"CPTCodes\"),\n",
    "            col(\"SubmissionChannel\"),\n",
    "            col(\"ServiceDate\"),\n",
    "            col(\"ClaimDate\")\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"FraudRisk\",\n",
    "        when(col(\"FraudScoreNumeric\") >= 10, lit(\"HIGH\"))\n",
    "        .when(col(\"FraudScoreNumeric\") >= 5, lit(\"MEDIUM\"))\n",
    "        .otherwise(lit(\"LOW\"))\n",
    "    )\n",
    "\n",
    "    return df.dropDuplicates([\"ClaimID\", \"MemberID\", \"ProviderID\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "380519cb-8bef-4e18-8809-886d015830a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting views to delta tables"
    }
   },
   "outputs": [],
   "source": [
    "silver_views = [\"silver_members\", \"silver_claims\", \"silver_providers\", \"silver_diagnosis_ref\"]\n",
    "\n",
    "for view in silver_views:\n",
    "    full_view_name = f\"capstone.medisure.{view}_view\"\n",
    "    new_table_name = f\"capstone.medisure.{view}_delta_table\"\n",
    "\n",
    "    # Check if the view exists via information_schema\n",
    "    exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM system.information_schema.tables\n",
    "        WHERE table_schema = 'medisure'\n",
    "          AND table_name = '{view}_view'\n",
    "          AND table_catalog = 'capstone'\n",
    "    \"\"\").collect()[0][0]\n",
    "\n",
    "    if exists > 0:\n",
    "        df = spark.table(full_view_name)\n",
    "        df.write.mode(\"overwrite\").saveAsTable(new_table_name)\n",
    "        print(f\"Converted {view} into {view}_table (Delta table)\")\n",
    "    else:\n",
    "        print(f\"View {full_view_name} does not exist, skipping...\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cs_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
